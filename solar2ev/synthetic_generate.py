#!/usr/bin/python

##################################
# use guide and API definition
# https://sdv.dev/SDV/user_guides/index.html
##################################

debug = False  # test with small dataset

"""
A common goal is to replace the real data with synthetic data for ML development. 
In this case, you can train using the synthetic data and test using the real data.* 
This is also known as the TSTR (Train Synthetic Test Real) score.

Another goal might be to enhance or augment the real dataset for ML development. 
In this case, you can train on a combined set of real and synthetic data, and then test with other real data*

*Keep in mind that the synthetic data is itself created using real data. For the most unbiased measurement, 
test the ML efficacy using real data that was not involved in the synthetic data creation.
"""

###### move to 1.9


# https://datacebo.com/blog/

# The Synthetic Data Gym (SDGym) is a Python library for benchmarking different synthetic data generators
# pip install sdgym
# https://docs.sdv.dev/sdgym/


# https://towardsdatascience.com/creating-synthetic-time-series-data-67223ff08e34
# https://github.com/gretelai/gretel-synthetics

import os
import sys
from time import perf_counter
import pandas as pd
import datetime
import copy

import numpy as np
from scipy.stats import shapiro

import pprint

sys.path.insert(1, '../my_modules') # this is in above working dir 
try:
    from my_decorators import dec_elapse
    from my_decorators import dec_elapse_arg

except Exception as e:
    print('%s cannot import modules in ..  check if it runs standalone. syntax error will fail the import' %__name__)
    exit(1)

import feature_plot
import config_model
import config_features # validate syn features are compatible with real features
import enphase # installation date
import calendar



# pip install sdv 
# pip install kaleido to save image

import sdv as sdv
from sdv.datasets.local import load_csvs
from sdv.metadata import SingleTableMetadata
from sdv.sequential import PARSynthesizer
from sdv.evaluation.single_table import run_diagnostic
from sdv.evaluation.single_table import evaluate_quality

# installed as part of sdv
import sdmetrics

from sdmetrics.reports.single_table import QualityReport
from sdmetrics.reports.single_table import DiagnosticReport

from sdmetrics.visualization import get_column_plot
from sdmetrics.visualization import get_column_pair_plot

# some metrics
from sdmetrics.single_column import CategoryCoverage
from sdmetrics.single_column import RangeCoverage
from sdmetrics.single_column import BoundaryAdherence
from sdmetrics.single_column import StatisticSimilarity
from sdmetrics.single_column import KSComplement

from sdmetrics.column_pairs import ContingencySimilarity
from sdmetrics.column_pairs import CorrelationSimilarity

from sdmetrics.single_table import NewRowSynthesis


# beta
from sdmetrics.timeseries import LSTMClassifierEfficacy 
from sdmetrics.single_table import LinearRegression, MLPRegressor
from sdmetrics.single_table import BinaryAdaBoostClassifier, BinaryDecisionTreeClassifier, BinaryLogisticRegression, BinaryMLPClassifier 
from sdmetrics.single_table import MulticlassDecisionTreeClassifier, MulticlassMLPClassifier

from rdt.transformers import FloatFormatter
from rdt import HyperTransformer


retain = config_model.retain
days_in_seq = config_model.days_in_seq  

######################
# file name
######################
# store all sdv artifacts 
sdv_dir = config_model.sdv_dir

# synthetic generated by sampling from trained sdv model, postprocessed.
# used by solar2ev
synthetic_data_csv =config_model.synthetic_data_csv

# training
sdv_model_file = os.path.join(sdv_dir, "sdv_trained_model.pkl")
real_data_csv = os.path.join(sdv_dir,"real_data.csv") # save in case

# sampling
sampled_data_csv = os.path.join(sdv_dir,"sampled_data.csv") # output of sampling

# quality, diagnostics report, 
quality_report_pkl = os.path.join(sdv_dir, 'quality_report.pkl')
diagnostic_report_pkl = os.path.join(sdv_dir, 'diagnostic_report.pkl')

#########################
# constraints
# not supported by PAR yet
#########################
# UserWarning: The PARSynthesizer does not yet support constraints. This model will ignore any constraints in the metadata.
# The SDV has 9 predefined constraint classes

# https://docs.sdv.dev/sdv/reference/constraint-logic
# Unique, FixedCombinations, Inequality, OneHotEncoding, FixedIncrements, Range, ..

production_positive = {
    'constraint_class': 'Positive',
    'constraint_parameters': {
        'column_name': 'production',
        'strict': True
    }
}

production_scalar_inequality = {
    'constraint_class': 'ScalarInequality',
    'constraint_parameters': {
        'column_name': 'production',
        'relation': '<=',
        'value':  30.0
    }
}

temp_scalar_range = {
    'constraint_class': 'ScalarRange',
    'constraint_parameters': {
        'column_name': 'temp',
        'low_value': -13.0,
        'high_value': 32.0,
        'strict_boundaries': False
    }
}

pressure_scalar_range = {
    'constraint_class': 'ScalarRange',
    'constraint_parameters': {
        'column_name': 'pressure',
        'low_value': 975.0,
        'high_value': 1050.0,
        'strict_boundaries': False
    }
}

constraints_list = [production_positive, production_scalar_inequality, temp_scalar_range, pressure_scalar_range]

########################
# custom constraints logic
########################

# If the predefined constraint classes don't meet your needs, you can write your own custom business logic
# https://docs.sdv.dev/sdv/reference/constraint-logic/custom-logic

from sdv.constraints import create_custom_constraint_class

# implement is_valid(), transform(), reverse_transform()

solar_c = "production"


###############
# GOAL: train sdv model from real data
# store trained model in sdv_model_file (.pkl), return synthetiser
###############

@dec_elapse
def train_synthetic(real_data:pd.DataFrame, metadata, epochs=128):

    # https://docs.sdv.dev/sdv/sequential-data/data-preparation


    print("\nSDV: TRAIN model on real data: %s. %d days. %d epochs" %(real_data.columns.to_list(), len(real_data)/24, epochs))

    # enforce min max
    #The synthetic data will contain numerical values that are within the ranges of the real data.
    #The synthetic data may contain numerical values that are less than or greater than the real data. 
    #Note that you can still set the limits on individual columns using Constraints.


    # Context columns do not vary inside of a sequence. For example, a user's 'Address' may not vary within a sequence 
    # while other columns such as 'Heart Rate' would. Defaults to an empty list.
    # at sampling time, can pass a "template" for what to use there

    # NO CONTEXT COLUMNS FOR US ?
    context = []

    # https://docs.sdv.dev/sdv/sequential-data/modeling/parsynthesizer

    # The PARSynthesizer is designed to work on multi-sequence data, which means that there are multiple sequences 
    # (usually belonging to different entities) present within the same dataset. This means that your metadata should 
    # include a sequence_key. 
    # Using this information, the PARSynthesizer creates brand new entities and brand new sequences for each one.

    par_synthesizer = PARSynthesizer(metadata, 
                                enforce_min_max_values=True,
                                enforce_rounding=True, 
                                context_columns=context,
                                epochs=epochs, 
                                verbose=True, 
                                cuda=True,
                                sample_size=2,
                                segment_size=None)
    
    # It learns how to create brand new sequences of multi-dimensional data, 
    # by conditioning on the unchanging, context values.


    ######################
    # add constraints
    # 1.10 UserWarning: The PARSynthesizer does not yet support constraints. This model will ignore any constraints in the metadata.
    ######################

    #par_synthesizer.add_constraints(constraints=constraints_list)

    ################################
    # quick segway on RDT (Reversible Data Transforms)
    ################################
    # NOTE: under the hood with sdv
    # The RDT library is a collection of objects that can understand your raw data convert it into cleaned, numerical data. 
    # publicly available Python library that translates between real world data and cleaned, numerical data that's ready for data science
    # numerical, categorical, datetome, text, pii (phone, email , ..), anonymize
    # https://docs.sdv.dev/rdt/transformers-glossary/numerical

    ## looking at data using RDT
    ht = HyperTransformer()
    ht.detect_initial_config(data=real_data)
    t = ht.get_config()
    print("RDT view of real data\n", t)
    # "sdtypes": {"temp": "numerical",,  "transformers": {"temp": FloatFormatter(),

    # https://colab.research.google.com/drive/1hShKLGrdIS7kmaSSTA0TGtLMtwx5CsWq?usp=sharing#scrollTo=t6OZmjyn2W5E

    # update transformers
    # https://docs.sdv.dev/rdt/usage/hypertransformer/configuration
    # ht.update_sdtypes(column_name_to_sdtype={'last_login': 'datetime','email_optin': 'categorical})

    # In the fit stage, the HyperTransformer references the config you set in the previous step while learning from your data values.
    # ht.fit(customers)

    # transform method to transform all the columns in your dataset at once.
    # transformed_customers = ht.fit_transform(customers)


    ##################
    # SDV preprocessing
    # using RDT under the hood
    #################

    # The SDV will pre and post-process your data by using reversible data transformations for each column
    # All synthesizers pre-process your data to prepare it for machine learning and 
    # then post-process synthetic data to convert it into the original format. 
    # https://docs.sdv.dev/sdv/single-table-data/modeling/customizations/preprocessing
    
    # Let the synthesizer auto assign the transformations based on the data you'd like to use for modeling.
    # column is not assigned to None, meaning that the data will not undergo any transformation for the machine learning model.
    par_synthesizer.auto_assign_transformers(real_data)

    # see what is used
    t = par_synthesizer.get_transformers()
    print("sdv auto assigned transformation:\n")
    pprint.pprint(t)
    # note that sequence_key is anonymized, and no transform for the rest
    # {'temp': None, 'year': AnonymizedFaker(function_name='bothify', function_kwargs={'text': '#####'}), 'pressure': None, 'production': None}

    ##########################
    # update transformers in case we are not happy with auto detect
    # do not anonymyze year (keep it as it)
    # rounding 
    ##########################
    par_synthesizer.update_transformers(column_name_to_transformer= {'year': None})

    # https://docs.sdv.dev/rdt/transformers-glossary/numerical/floatformatter
    par_synthesizer.update_transformers(column_name_to_transformer= {'production': FloatFormatter(learn_rounding_scheme=True)})
    par_synthesizer.update_transformers(column_name_to_transformer= {'temp': FloatFormatter(learn_rounding_scheme=True)})

    t = par_synthesizer.get_transformers()
    print("sdv columns updated transformation:\n")
    pprint.pprint(t)
    #  {'temp': None, 'year': None, 'pressure': FloatFormatter(learn_rounding_scheme=True), 'production': FloatFormatter(learn_rounding_scheme=True)}


    # NOTE: can directly use par_synthesizer.fit(real_data)  = preprocess and fit in one operation
    # or 2 separate steps (process + fit)

    #processed_data = par_synthesizer.preprocess(real_data) #  preprocess the data according to the transformations. 
    #par_synthesizer.fit_processed_data(processed_data) # model training on preprocessed data. progress bar, Loss , 30sec

    # config of PAR
    param = par_synthesizer.get_parameters()
    # {'enforce_min_max_values': True, 'enforce_rounding': True, 'locales': None, 'context_columns': [], 'segment_size': None, 'epochs': 256, 'sample_size': 2, 'cuda': True, 'verbose': True}

    # metadata
    met = par_synthesizer.get_metadata()

    #################
    # train sdv model
    #################
    print("\nTRAINING SDV:")
    tmp = perf_counter() # always returns the float value of time in seconds.  Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration.   
    

    #######################
    # WTF. exception with sdv 1.10. ok with 1.9. anyway constraints for time series still not there in 1.10
    #######################
    par_synthesizer.fit(real_data) # progress bar Loss, in nb of epochs
    
    e = perf_counter() - tmp
    print('SDV model fitted in %0.0f sec. %0.0f sec/epochs' %(e, e/epochs))

    print("SDV model saved to ", sdv_model_file)
    par_synthesizer.save(filepath=sdv_model_file)

    return(par_synthesizer)


###############
# GOAL: create synthetic data (sample)
###############

@dec_elapse_arg("elapse on sampling synthetic data")
def sample_synthetic(synthesizer, num_sequences=1, sequence_length=None):

    print("\nSDV: SAMPLE %d sequence of length %s" %(num_sequences, str(sequence_length))) #  len can be None

    # You can pass in context columns and allow the PAR synthesizer to simulate the sequence based on those values.
    # https://colab.research.google.com/drive/1YLk2uwn8yrSRPy0soEeJwu8Hdk_tGTlE?usp=sharing#scrollTo=_8l_WLg-KhLH
    # scenario_context = pd.DataFrame(data={ 'Symbol': ['COMPANY-A', 'COMPANY-B', 'COMPANY-C', 'COMPANY-D', 'COMPANY-E'],'Sector': ['Technology']*2 + ['Consumer Services']*3,'Industry': ['Computer Manufacturing', 'Computer Software: Prepackaged Software']})
    scenario_context = []

    tmp = perf_counter()

    synthetic_data = synthesizer.sample(
        num_sequences=num_sequences, 
        sequence_length=sequence_length) # progress bar, num seq
    
    # 1506 sec per sequence 25mn
    # Table containing the sampled sequences in the same format as that he training data had.

    e = perf_counter() - tmp

    # 1506 sec per sequence
    print('synthetic data sampled in %0.0f sec. %0.1f mn %0.0f sec per sequence' %(e, e/60,  e/num_sequences))
    print("%d samples generated" %len(synthetic_data))

    assert len(synthetic_data) == num_sequences * sequence_len
    
    # nb row is seq len * num_seq
    # nb columns is nb features in real data

    # generated sequence key (year) column is the same value within one sequence. it is anonymized by default

    #production,year,temp,pressure
    #14.21,71867,1.2896466593834282,1019.05

    ##########################
    # post process syn data
    ##########################
    ## no need to manually clean
    # set negative production to zero
    #syn_data["production"] = syn_data["production"].apply(lambda x: 0.2 if x<0 else x) # beware of bin 
    #cap = 35
    #syn_data["production"] = syn_data["production"].apply(lambda x: x if x< cap else cap)

    if sequence_length is not None:
        assert synthetic_data.shape[0] == sequence_length * num_sequences

    assert synthetic_data.isnull().sum().sum() == 0
    assert synthetic_data.isnull().sum().all() == 0 # another way

    ##########################
    # validate syn data is "compatible" with real data for later use
    # as "combination" is done at dataset (of sequence) level, we just need to make sure it has the same features
    #########################

    return(synthetic_data)



def sdv_report_plot_analyze(real_data, synthetic_data, metadata):


    ##################
    # quality
    ##################
    # The Quality Report is not optimized to compute the quality of ordered, sequential information. 
    # However, you can still apply the single table report for some basic analysis.
    # The Quality Report checks for statistical similarity between the real and the synthetic data. 
    # Use this to discover which patterns the synthetic data has captured from the real data.

    # The SDMetrics Quality Report evaluates how well your synthetic data captures mathematical properties from your real data. 
    # This is also known as synthetic data fidelity.

    # https://docs.sdv.dev/sdv/single-table-data/evaluation/data-quality

    # print some Generating report ... overall shape and pair trends
    quality_report = evaluate_quality(real_data, synthetic_data, metadata) 


    # per colums or colums pair
    # score per colums (ie temp, pressure, production) or per columns pair
    # KsComplement for shape, CorrelationSimilarity for pait trend
    for x in ["Column Shapes" , "Column Pair Trends"]:
        print("sdv detailed (per column) quality for: %s\n" %x, quality_report.get_details(property_name=x))



    ##################
    # diagnostics
    ##################
    # https://docs.sdv.dev/sdv/single-table-data/evaluation/diagnostic
    # similar info as with sdm directly
    # print on console, returns object, 
    # Data validity, data structure score should be 100%
    diag_report = run_diagnostic( real_data=real_data, synthetic_data=synthetic_data, metadata=metadata)


    # Data Validity per colums (BoundaryAdherence)
    # Data Structure: per table  TableStructure
     # Continuous values in the synthetic data must adhere to the min/max range in the real data , Discrete values in the synthetic data must adhere to the same categories as the real data.
    # BoundaryAdherence for all columns

    # Checks to ensure the real and synthetic data have the same column names
    # TableStructure
    for x in ["Data Validity" , "Data Structure"]:
        print("sdv quality for %s\n" %x, diag_report.get_details(property_name=x))

    ############################
    # compare real and synthetic data
    ############################

    # count, min, max, quentile
    #print("original data:\n", real_data.describe())
    #print("synthetic data:\n", synthetic_data.describe())

    print("\nstatistics")
    # look at production. original data
    d = real_data["production"]
    print("original data production: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(d.max(), d.min(), d.mean(), d.std()))

    # look at production. synthetic data
    d = synthetic_data["production"]
    print("synthetic data production: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(d.max(), d.min(), d.mean(), d.std()))


    # https://docs.sdv.dev/sdv/single-table-data/evaluation/visualization

    # get_column_plot Use this utility to visualize a real column against the same synthetic column.
    # get_column_pair_plot Use this utility to visualize the trends between a pair of columns for real and synthetic data.
    # get_cardinality_plot Use this utility to visuaize the cardinality of parent-child relationship.
    
    # visualize a real column against the same synthetic column. 
    # You can plot any column of type: boolean, categorical, datetime or numerical. 
    # distplot or bar (discrete)

    for c in ["production", "pressure", "temp"]: 
        fig = get_column_plot(real_data= real_data, synthetic_data=synthetic_data, column_name=c, plot_type = 'distplot') # distribution
        fig.show() # create tab , fig3 is <class 'plotly.graph_objs._figure.Figure'>, looks like dict
        f = os.path.join(sdv_dir, c +".png")
        print("saving %s" %f)

        #### WTF; this hangs
        #fig.write_image(f) # save plotly


    # visualize the trends between a pair of columns for real and synthetic data
    # 'scatter': Plot each data point in 2D space as a scatter plot. Use this to compare a pair of continuous columns.
    #'box': Plot the data as one or more box plot. Use this to compare a continuous column with a discrete column.
    #'heatmap': Plot a side-by-side headmap of the data's categories. Use this to compare a pair of discrete columns.

    for c in ["temp", "pressure"]:
        fig = get_column_pair_plot(real_data=real_data,synthetic_data=synthetic_data, column_names=[c, 'production'], plot_type = 'scatter') # 2D , z is value
        fig.show()
        f = os.path.join(sdv_dir, c +"_production.jpeg")
        print("saving %s" %f)

        #fig.write_image(f)



########################
# GOAL: checks for statistical similarity between the real and the synthetic data. 
# Column Shapes, Column Pair Trends
########################

def sdm_quality_report(real_data, synthetic_data, metadata):

    # https://docs.sdv.dev/sdmetrics/reports/quality-report


    # NOTE same data as sdv
    report = QualityReport()
    print("NOTE: same data as sdv reports")
    report.generate(real_data, synthetic_data, metadata.to_dict()) # will print some

    print("SDM overall quality score: %0.1f" %report.get_score()) # A floating point value between 0 and 1 that summarizes the quality of your synthetic data.

    print("SDM quality properties ie columns shape/distribution and correlation/trends:\n", report.get_properties()) 

    # get result per colums
    for x in ["Column Shapes", "Column Pair Trends"]: 
        if x == "Column Shapes":
            print("%s, ie distribution\n" %x , report.get_details(x))
        else:
            print("%s, ie correlation\n" %x, report.get_details(x))


    # SDM visualization
    # https://docs.sdv.dev/sdmetrics/reports/visualization-utilities

    # creates tab in browser
    for p in ["Column Shapes", "Column Pair Trends"]:
        fig1 = report.get_visualization(property_name=p) # bar chart
        fig1.show()

        # WTF hangs
        #fig1.write_image(os.path.join(sdv_dir, p +".jpeg"))
       
    
    # save the metadata along with the score for each property, breakdown and metric.
    # share or access it in the future.
    report.save(filepath=quality_report_pkl)
    
    return()


##########################
# GOAL: data validity and structure
#########################
def sdm_diagnostic_report(real_data, synthetic_data, metadata):

    # 'Data Validity' or 'Data Structure'.
    # same as with sdv, except can plot some
    # https://docs.sdv.dev/sdmetrics/reports/diagnostic-report/single-table-api
    # https://docs.sdv.dev/sdmetrics/reports/diagnostic-report/whats-included

    report = DiagnosticReport()
    report.generate(real_data, synthetic_data, metadata.to_dict())
    print("sdm quality report overall score %0.1f" %report.get_score())
    print("sdm quality:\n" , report.get_properties())
    print(report.get_details(property_name='Data Validity'))
    print(report.get_details(property_name='Data Structure'))

    #https://docs.sdv.dev/sdmetrics/reports/visualization-utilities

    for x in ['Data Validity']: # Data Structure does not have a visualization
        fig = report.get_visualization(property_name=x)
        fig.show()
    
    report.save(filepath=diagnostic_report_pkl)


#############################
# GOAL: sdm metrics
#############################

# You can compute additional metrics using our standalone SDMetrics library.
# If you're used to older versions of the SDV, you may be looking for NewRowSynthesis, CategoryCoverage, and RangeCoverage.  
# https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/newrowsynthesis


def sdm_metrics(real_data, synthetic_data, metadata):

    # https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary
    # We recommend applying the metrics using SDMetrics Reports. Reports determine which exact metrics to use based on your data.
    
    # metrics included in quality , diagnostic report, or other

    print("\nadditional sdm metrics")
    
    # measures whether each row in the synthetic data is novel, or whether it exactly matches an original row in the real data.
    m = NewRowSynthesis.compute(
    real_data=real_data,
    synthetic_data=synthetic_data,
    metadata=metadata.to_dict(),
    numerical_match_tolerance=0.01,
    synthetic_sample_size=None)

    # (best) 1.0: The rows in the synthetic data are all new. There are no matches with the real data.
    # (worst) 0.0: All the rows in the synthetic data are copies of rows in the real data.
    print('new row synthesis %0.1f' % m)

    # computes the min and max values of the real column. 
    # Then, it computes the frequency of synthetic values that are in the [min, max] range.
    c = "production"
    m = BoundaryAdherence.compute(
    real_data=real_data[c],
    synthetic_data=synthetic_data[c])
    print("boundary adherance for %s %0.1f" %(c, m))

    # measures whether a synthetic column covers the full range of values that are present in a real column.
    # (best) 1.0: The synthetic column covers the range of values present in the real column
    # (worst) 0.0: The synthetic column does not overlap at all with the range of values in the real colum
    c = "production"
    m = RangeCoverage.compute( real_data=real_data[c], synthetic_data=synthetic_data[c])
    print("range coverage for %s %0.1f" %(c, m))


    # measures the correlation between a pair of numerical columns and computes the similarity between the real and synthetic data -- 
    # aka it compares the trends of 2D distributions. 
    # numerical and datetime
    # https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity
    # the real data has a strongly positive correlation of 0.93 but the synthetic data has a weak correlation of 0.22. The overall similarity score is 0.64, capturing the fact that the synthetic data has a noisier trend.
    c1 = [['production', 'pressure'], ['production', 'temp']]
    for c in c1:
        m  =  CorrelationSimilarity.compute(
        real_data=real_data[c],
        synthetic_data=synthetic_data[c],
        coefficient='Pearson')
        print("pairwise %s correlation similarity of the real synthetic data %0.1f" %(c, m))

    # for CATEGORICAL or BOOLEAN
    # similarity of a pair of categorical columns between the real and synthetic datasets -- aka it compares 2D distributions.

    # measures the similarity between a real column and a synthetic column by comparing a summary statistic. 
    #c1 = [['production', 'pressure'], ['production', 'temp']]
    #for c in c1:
    #    m = ContingencySimilarity.compute(
    #    real_data=real_data[c],
    #    synthetic_data=synthetic_data[c])
    #    print("pairwise correlation CATEGIRICAL of the real/synthetic data similarity for %s %0.1f" %(c, m))

    c = 'production'
    s = ['std', 'mean', 'median']
    for s1 in s:
        m =  StatisticSimilarity.compute(
        real_data=real_data[c],
        synthetic_data=synthetic_data[c],
        statistic=s1)
        print("%s similarity for %s %0.1f" %(s1, c, m))

    # computes the similarity of a real column vs. a synthetic column in terms of the column shapes -- aka the marginal distribution or 1D histogram of the column.
    # Recommended Usage: The Quality Report applies this metric every compatible column and provides visualizations to understand the score. 
    c1 = ["production", "temp", "pressure"]
    for c in c1:
        m = KSComplement.compute(
        real_data=real_data[c],
        synthetic_data=synthetic_data[c])
        print("KS complement (1D histogram, aka column shapes) comparison for %s %0.1f" %(c, m))

    # TVcomplement for categorical and boolean
        

def sdm_ml_metrics(real_data, synthetic_data, metadata):

    print("\ncompute ML metrics")

    #####################
    # ML Efficacy 
    # metrics that calculate the success of using synthetic data to perform an ML prediction task.
    ######################

    c = "production"

    # categorical Multiclass Classification
    # boolean columns Binary Classification
    # numerical Regression 


    ### single table: regression
    # target should be numerical

    m = LinearRegression.compute(
    test_data=real_data,
    train_data=synthetic_data,
    target=c,
    metadata=metadata.to_dict())
    print("ML Efficacy: single table, regression %s %0.1f" %(c, m))

    ### single table: Multiclass Classification

    try:
        m = MulticlassMLPClassifier.compute(
        test_data=real_data,
        train_data=synthetic_data,
        target=c,
        metadata=metadata.to_dict())

        print("ML Efficacy: single table, MultiClass classification %s %0.1f" %(c, m))
    
    except Exception as e:
        print("Exception: ML Efficacy:  MultiClass classification. %s" %str(e)) # Unknown label type:



    # sequential
    # LSTMClassifierEfficacy calculates the success of using synthetic data to perform an ML prediction task
    # https://docs.sdv.dev/sdmetrics/metrics/metrics-in-beta/ml-efficacy-sequential

    # target: A string representing the name of the column that you want to predict.
    # The target column must be discrete (categorical or boolean).

    
    try:

        m = LSTMClassifierEfficacy.compute(
        real_data=real_data,
        synthetic_data=synthetic_data,
        metadata=metadata.to_dict(),
        target=c,
        sequence_key='year')

        print("ML Efficacy: Sequential (LSTM) %s %0.1f" %(c, m))
    except Exception as e:
        print("Exception: ML Efficacy: Sequential (LSTM). %s" %str(e)) #  y contains previously unseen labels: [0.26]




####################################
# GOAL: benchmarking different synthetic data generators
####################################
def gym():

    # WARNING: does not seem to support sequential (ie PAR)

    # pip install sdgym anyio
    print("\nrunning sdgym")

    import sdgym
    sdgym_folder = "sdgym"

    # The Synthetic Data Gym (SDGym) is a Python library for benchmarking different synthetic data generators. 
    # For example you can compare synthesizers that use classical statistics versus those that use deep learning.

    #### custom dataset

    # https://docs.sdv.dev/sdgym/customization/datasets/custom-datasets
    # https://docs.sdv.dev/sdgym/resources/metadata

    #Compress the CSV and JSON file for each dataset into a single zip file
    #Put all the zip files into a single folder for all your custom datasets

    # The path to your folder that contains additional datasets. 
    # MUST contain real_data.zip (and no .csv or .json)
    # NOTE: same metadata.json as saved earlier (after autodetect and update)
    my_dataset_folder = os.path.join(sdgym_folder, "dataset") 
    print("using custom dataset (.zip) in dir: %s" %my_dataset_folder)

    ##### results files

    # output_file_path Save the the final results at this location. The results are available as a csv file,
    #### NOTE: output_file and the pandas result are the same
    ouput_file = os.path.join(sdgym_folder, "sdgym_output.csv") # param 

    try:
        os.remove(ouput_file) 
    except Exception as e:
       pass

    # dataframe returned from benchmarck
    result_file = os.path.join(sdgym_folder, "sdgym_results.csv")

    # Store the detailed results (as multiple files) within the provided folder
    detailed_results_folder = os.path.join(sdgym_folder, "detailed")

    try:
        os.removedirs(detailed_results_folder) # sdgym\detailed already exists. Please provide a folder that does not already exist.
        print("removed %s" %detailed_results_folder)
    except Exception as e:
        print("warning: cannot remove %s %s" %(detailed_results_folder, str(e)))

    # [Errno 2] No such file or directory: 'sdgym\\metadata.json\\metadata.json.csv'


    #### run gym
        
    #The FAST_ML preset is our first preset. It uses machine learning (ML) to model your data while optimizing for the modeling time. 
    # This is a great choice if it’s your first time using the SDV for a large custom dataset or if you’re exploring the benefits of using ML to create synthetic data.



    # https://docs.sdv.dev/sdgym/customization/synthesizers/sdv-synthesizers
        
    # https://sdv.dev/SDV/user_guides/single_table/tabular_preset.html
    synthesizer_list = ['CTGANSynthesizer', 'CopulaGANSynthesizer']
    synthesizer_list = ['FastMLPreset', 'TVAESynthesizer', 'GaussianCopulaSynthesizer']

    # FastML 1sec
    # TVAE 115sec
    # gaussianCopula 3sec
    # GAN based ?? LOONG

    #Synthesizer,Dataset,Dataset_Size_MB,Train_Time,Peak_Memory_MB,Synthesizer_Size_MB,Sample_Time,Evaluate_Time,Quality_Score,

    #NewRowSynthesis, MissingValueSimilarity,RangeCoverage,BoundaryAdherence,CorrelationSimilarity

    my_sdmetrics = [
        ('NewRowSynthesis', {'synthetic_sample_size': 10}),
        'MissingValueSimilarity',
        'RangeCoverage',
        'BoundaryAdherence',
        ('CorrelationSimilarity', {'coefficient': 'Spearman'})]

    # https://docs.sdv.dev/sdgym/benchmarking/running-a-benchmark
    print("\n===> starting sdgym benchmark")

    # progress bar in number of synthetizer


    results = sdgym.benchmark_single_table(synthesizers = synthesizer_list, 
                                           sdv_datasets=[], 
                                           additional_datasets_folder = my_dataset_folder,
                                           compute_quality_score = True,show_progress = True,
                                           output_filepath = ouput_file, 
                                           sdmetrics = my_sdmetrics)


    # https://docs.sdv.dev/sdgym/benchmarking/interpreting-results
    print("sdgym: result dataframe:\n", results)
    results.to_csv(result_file)
    # A result for each dataset and synthesizer will be able after the benchmarking finishes.
    # result and output_file the same
    return(result_file)


################################
# GOAL: add year column to use as sequence key 
# year 1 is 1st 365 days of real data, etc ..
# year is derived from date
# drop date
################################
def convert_date_to_year(real_data, sequence_key_col):

    # convert date (str) to datetime
    #real_data["date"] = pd.to_datetime(real_data["date"], format='ISO8601')
    # WTF time data '2021-03-11' does not match format 'ISO8601' (match)

    real_data["date"] = pd.to_datetime(real_data["date"], format='%Y-%m-%d')  # dtype object to dtye datetime64[ns]

    # convert date in 2x365 days into some unique id, year 1 and year 2
    # year 1 includes days from both calendar year 2021 and 2022, year 1 is April 1st 2021 onward

    d1 = real_data["date"].iloc [0] # Timestamp('2021-04-01 00:00:00') 
    d2 = d1 + datetime.timedelta(365)

    def get_year(d):
        if d < d2:
            return(1) # means year 1, not 2021
        else:
            return(2)
        
    #real_data.insert(1,"year", df["date"].apply(lambda x: 1 if x.year == 2021 else 2 ), False )
    real_data.insert(1, sequence_key_col, real_data["date"].apply(get_year), False )

    # drop date columns
    real_data.drop('date', axis='columns', inplace=True)

    return(real_data)


###########################
# GOAL: select columns of real data to generate as synthetic
# add year and select sequence key
# build metadata
###########################

installation_month = None

def build_metadata_realdata(features_input_csv, features_used):

    global installation_month

    # https://jdhao.github.io/2022/07/27/pass_list_by_value_python/
    features = copy.deepcopy(features_used) # otherwize, will modifie in caller

    # WARNING: colums selection should be compatible with real data features 

    #######################
    # what to use for training
    #  features, ammount of real data, sequence column
    #  see discusion below
    ######################
    #f= ["temp", "pressure", "production"]  # rather get from head definition

    # # what to use as sequence key 
    sequence_key_col = "year" # ie generated synthetic "year" (a separate year column will be created from date)

    # typically in features as sin and cos
    # not sure if good idea, there is nothing like an "average" month (average year exists)
    # could generate a bunch of synthetic jan, synthetic feb etc .. and reassemble. this is complex for nothing. year is better
    # sequence_key_col = "month" 

    nb_years = 2 # ammount of real data to use in training
    assert nb_years >= 2


    # https://docs.sdv.dev/sdv/sequential-data/data-preparation
    # https://docs.sdv.dev/sdv/sequential-data/data-preparation/sequential-metadata-api
    # metadata = singletable + set_sequence_key + set_sequence_index

    # load existing real data  to train sdv
    try:
        df_model =  pd.read_csv(features_input_csv)
    except Exception as e:
        print("%s must already exist to use vault %e" %(features_input_csv, str(e)))
        sys.exit(1)


    #########################
    # statistic on real data
    #########################

    c = "production"
    d = df_model[c]

    print("real production (before training sdv): max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(d.max(), d.min(), d.mean(), d.std()))
    
    (_,p) = shapiro(d)
    if p<0.05:
        print("%s AFTER normalization is not normal" %c)
    else:
        print("%s AFTER normalization is normal" %c)

    for c in df_model.columns:
        if c in ["temp", "humid", "pressure", "wind"]:

            d = df_model[c]

            print("real data %s: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(c, d.max(), d.min(), d.mean(), d.std()))

            (_,p) = shapiro(d)
            if p<0.05:
                print("%s is not normal" %c)
            else:
                print("%s is normal" %c)


    ###############################
    # feature selection for synthetic data 
    ############################### 

    # option 1: get all features from all heads of CURRENT model , EXCEPT date
    #data = df_model.drop(["date"], axis=1) # drop columns
    
    # option 2: ease sdv life by minimizing number of colums created. 
    # months and hour can be recreated after the fact
    #data = df_model.drop(["date", "cos_hour","sin_hour","sin_month", "month", "hour"], axis=1) # drop columns

    # option 3: sin/cos can be recreated after the fact. examine how good created month and hour colums are

    # ===> option 4: do no use month, hour, cos/sin when using sdv. restrict to basic features

    # NOTE: need a sequence_key columns

    ##############################################
    # set sequence key

    # looks like sdv sequential model need a sequence key colums
    # The sequence key is a column that identify which row(s) belong to which sequences
    # "sequence_key": A column name of the sequence key, if you have multi-sequence data
    # must be an ID or another PII sdtype.

    # The PARSynthesizer is designed to work on multi-sequence data, which means that there are multiple sequences 
    # (usually belonging to different entities) present within the same dataset. 
    # This means that your metadata should include a sequence_key. Using this information, 
    # the PARSynthesizer creates brand new entities and brand new sequences for each one

    # shit, Note: The SDV sequential models do not fully support single sequence data.

    # could use month columns
    # or add a year columns (but then need to includes at least two years)
    #############################################


    # do not generate what can be recreated after the fact: hour, month and trig version
    for e in ["sin_month", "cos_month", "month" , "sin_hour", "cos_hour" , "hour", "direction_cos", "direction_sin"]:

        try:
            features.remove(e)
        except:
            pass
    
    ####################################
    # make sure we have date colum, as year generated from date
    ####################################

    features.append("date") # will be replaced by year later. should not be in network input

    print("sdv: features used in sdv:", features)

    ########################
    # get n years of real data, ONLY FOR features which will be synthetized
    # starts at begining of a month
    ########################
    
    real_data = df_model[features]

    # start march 12. easier to later infer month from sheer sequence if I sample from April 1st, 
    installation = enphase.installation
    # installation = datetime.datetime(2021,3,12) 

    # how many days to add to the 13th of march to get to April 1st

    # number of days in march 2021 , ie 31
    d = calendar.monthrange(installation.year, installation.month) [1]
    
    # number of day beytween 12 and 31 (both included)
    n = d - installation.day +1 # ignore first n days
    n = n *24 # in hours

    real_data = real_data.iloc[n:n+24*365*nb_years]
    nb = len(real_data)

    installation_month = installation.month

    ########################
    # create year column from date and drop date
    ########################

    real_data = convert_date_to_year(real_data, sequence_key_col) # add year columns
    assert len(real_data) == nb

    print("sdv: real data used for training: %s. %d samples %d days" %(real_data.columns.to_list(), len(real_data), len(real_data)/24))

    # save real data to csv (in case)

    real_data.to_csv(real_data_csv, header=True, index=False)

    # just for fun, play with SDV load_csvs
    datasets = load_csvs(folder_name=sdv_dir)
    # dict_keys(['real_data']), # type(datasets['real_data']) <class 'pandas.core.frame.DataFrame'>

    # Returns A dictionary that contains all the CSV data found in the folder. The key is the name of the file (without the .csv suffix) 
    # and the value is a pandas DataFrame containing the data.


    #########################
    # build metadata
    #########################
    metadata = SingleTableMetadata()

    ### autodetect metadata
    metadata.detect_from_dataframe(data=real_data)

    ## sdtypes
    # All SDV models require information about the data type for every column. In the SDV, the data types are specified by sdtype, 
    # denoting a semantic or statistical meaning.
    # https://docs.sdv.dev/sdv/reference/metadata-spec/sdtypes
    # boolean , categorical, datetime, numerical, id, Domain-Specific Concepts , PII

    ########################################
    # update metadata if inaccurate
    ########################################

    # metadata.update_column( column_name='has_rewards', sdtype='boolean')
    # https://docs.sdv.dev/sdv/single-table-data/data-preparation/single-table-metadata-api

    try:
        metadata.update_column(column_name='date', sdtype = "datetime", datetime_format = "%Y-%m-%d")
    except Exception as e:
        pass

    #######################
    # set sequence key
    #######################
    # sequence columns must be id (not numerical)
    # sequence specific, https://docs.sdv.dev/sdv/reference/metadata-spec/sequential-metadata-json

    metadata.update_column( column_name=sequence_key_col, sdtype='id')
    metadata.set_sequence_key(column_name=sequence_key_col)

    ####################
    # "sequence_index" . NOT USED
    ####################
    # determines the spacing between the rows in a sequence. Use this if you have an explicit index such as a timestamp. 
    # If you don't supply a sequence index, the SDV assumes there is equal spacing of an unknown unit.
    # Any existing sequence indices will be removed.
    
    #metadata.set_sequence_key(sequence_index='Time')


    ### metadata to dict 
    metadata_dict = metadata.to_dict() # Note that the returned object is a representation of the metadata. Changing it will not modify the original metadata object in any way.
    # {'columns': {'month': {...}, 'temp': {...}, 'pressure': {...}, 'production': {...}}, 'sequence_key': 'month', 'METADATA_SPEC_VERSION': 'SINGLE_TABLE_V1'}
    print("metadata dict:")
    pprint.pprint(metadata_dict)
    
    ### creates pic with metadata
    # 'full', 'summarized
    # not very sexy pictures
    metadata.visualize( show_table_details='full',output_filepath=os.path.join(sdv_dir, "metadata_full.png"))
    metadata.visualize( show_table_details='summarized',output_filepath=os.path.join(sdv_dir, "metadata_summary.png"))


    ### validate metadata
    # validate that the metadata is written according to the specification
    metadata.validate() # throw error

    # validate that the metadata accurately describes a particular dataset.
    print("verify the metadata accurately describe real data")
    metadata.validate_data(data=real_data)

    ### save metatada to json
    # does not overwrite
    try:
        os.remove(os.path.join(sdv_dir, "metadata.json"))
    except Exception as e:
        pass
    finally: 
        # autodetectected + updated
        print("saving metadata to json")
        metadata.save_to_json(filepath=os.path.join(sdv_dir, "metadata.json"))
    
    return(metadata, real_data, sequence_key_col)



###########################################
# post process sampled data
###########################################

def post_process(sampled_data):


    print("drop year columns") # was sequence key
    syn_data = sampled_data.drop(["year"], axis=1) # drop columns

    #no need to add date, it is not in network input
    
    ########################################
    # add month if it was in network input
    ########################################

    i=set.intersection(set(["month", "cos_month", "sin_month"]),set(features_used)) # returns set

    if len(i) >0:
        print("month is used in network input, add month columns (incl trig version)")

        syn_data.insert(2,"month",0) 

        # 1st 30th entry are month April, etc
        # sampled data does not start Jan 1st, not the same number of days per months. all we have is a sequence 1 to 365*2*24
        # KISS. just change month. make sure real data used for training starts at month boundaries, ie April 1st 

        # apply's function gets a value, but I need the index of the row
        #real_data.insert(2,"month", real_data["year"].apply(insert_month) )

        current_month = installation_month  # 1st 30 entries will be tagged April
        current_month = current_month + 1 # mars to april
       
        #for index, row in syn_data.iterrows(): #  returns a tuple containing the index of the row and a Series object that contains the values for that row.
        for index in syn_data.index: # each row is one hour
            # just use months of 30 days
            if index !=0 and index % (24*30) == 0:
                # new month
                current_month = current_month +1
                if current_month >12:
                    current_month = 1
            else:
                pass

            #row["month"] = current_month
            #syn_data.iloc[index] ["month"] = current_month  # this does not update cells
            syn_data.at[index, "month"] = current_month


        ############
        # trig month
        ############
    
        i=set.intersection(set(["cos_month", "sin_month"]),set(features_used))
        if len(i) >0:
            print("trig month is used in network input, add trig month column")
            # month, represented as 1 to 12
            # max = 12 is the same as 0 , jan = 1 is close
            _max = 12 #  nb of different features, 

            # create new columns all at once
            syn_data['cos_month'] = round(np.cos(syn_data["month"]*(2.*np.pi/_max)),2)
            syn_data['sin_month'] = round(np.sin(syn_data["month"]*(2.*np.pi/_max)),2)


    ########################################
    # add hour if it was in network input
    ########################################

    i=set.intersection(set(["hour", "cos_hour", "sin_hour"]),set(features_used))

    if len(i) >0:
        print("hour is used in network input, add hour column")

        syn_data = syn_data.insert(2,"hour",0)

        # 1st 30th entry are month April, etc
        # sampled data does not start Jan 1st, not the same number of days per months. all we have is a sequence 1 to 365*2*24
        # KISS. just change month. make sure real data used for training starts at month boundaries, ie April 1st 

        # apply's function gets a value, but I need the index of the row
        #real_data.insert(2,"month", real_data["year"].apply(insert_month) )

        current_hour  = 0 

        for index, row in syn_data.iterrows():
            # just use months of 30 days
            if index % 24 == 0:
                # new day
                current_hour = 0 
            else:
                current_hour = current_hour +1
                
            syn_data.at[index, "hour"] = current_hour
            

        i=set.intersection(set(["cos_hour", "sin_hour"]),set(features_used))
        if len(i) >0:
            print("trig hour is used in network input, add trig hour column")
            _max = 24 #  nb of different features, 


            syn_data['cos_hour'] = round(np.cos(syn_data["hour"]*(2.*np.pi/_max)),2)
            syn_data['sin_hour'] = round(np.sin(syn_data["hour"]*(2.*np.pi/_max)),2)

    i=set.intersection(set(["direction_cos", "direction_sin"]),set(features_used))

    if len(i) >0:
        print("trig direction is used in network input, add trig direction column")

        import meteo 
        _max = len(meteo.direction) + 1 # nb of discrete +1

        # Insertion index. Must verify 0 <= loc <= len(columns).
        #real_data_csv.insert(len(df.columns), "sin_month", df["month"].apply(cyclical_value_to_trig, args=(_max, 'sin')))
        syn_data['direction_cos'] = round(np.cos(syn_data["direction"]*(2.*np.pi/_max)),2)
        syn_data['diection_sin'] = round(np.sin(syn_data["direction"]*(2.*np.pi/_max)),2)

    return(syn_data)


############
# generate SDV/SDM report and metrics on sampled data 
############

def all_reports(sampled_data):

    print("\ngenerate reports for sampled data\n")

    # SDV

    print("\nsdv reports:") # sdv
    sdv_report_plot_analyze(real_data, sampled_data, metadata)

    # SDM 

    print("\nsdm quality reports:")
    sdm_quality_report(real_data, sampled_data, metadata)

    print("\nsdm diagnostics reports:")
    sdm_diagnostic_report(real_data, sampled_data, metadata)

    print("\nadditional sdm metrics:")
    sdm_metrics(real_data, sampled_data, metadata)

    try:

        print("\nMLsdm metrics:")
        print(real_data.columns, sampled_data.columns)

        sdm_ml_metrics(real_data, sampled_data, metadata)
        #Feature names must be in the same order as they were in fit.
    except Exception as e:
        print(str(e))
    

    print("\nsdgym:")
    # NOTE: seems only for table, not sequential model
    #result_csv = gym()
    #print("gym done. look at: %s" %result_csv)

    #Synthesizer,Dataset,Dataset_Size_MB,Train_Time,Peak_Memory_MB,Synthesizer_Size_MB,Sample_Time,Evaluate_Time,Quality_Score,
    # NewRowSynthesis,   MissingValueSimilarity,RangeCoverage,BoundaryAdherence,CorrelationSimilarity


def check_syn_data():
    #################
    # load syntethic data just created , and check features
    #################

    
    print("loading synthetic data from %s" %synthetic_data_csv)
    try:
        df_model_syn =  pd.read_csv(synthetic_data_csv)
    except Exception as e:
        print("cannot load synthetic data %s" %str(e))
        raise Exception ("make sure to run synthetic data generation before using this option")

    # validate features in syn data are compatible with real data
    s =  df_model_syn.columns.to_list() # columns used in synthetic data
    print("synthetic data %s" %s)
    print("network inputs %s" %config_features.config["input_feature_list"]) # all heads

    # build list of features used by network
    f = []
    for c1 in config_features.config["input_feature_list"]:
        for c in c1:
            f.append(c)
    f = list(set(f)) # columns used in real data
    assert set(f).issubset(set(s)) , "real data is not a subset of synthetic data%s %s" %(f,s)

    print("synthetic data len: %d days: %d, years: %0.1f" %(len(df_model_syn), len(df_model_syn)/24, len(df_model_syn)/(24*365)))


############################################
# MAIN
############################################

#####################################
# meant to ONLY as standalone
# train sdv and sample synthetic data (OFFLINE)
# created sythetic data (csv) 
#  which is loaded in solar2ev to train LSTM model on dataset (real + syn)
######################################

if __name__ == "__main__":

    print("train sdv on real data and generate synthetic data")

    print("sdv:" , sdv.__version__)
    print("sdmetrics:" , sdmetrics.__version__)

    print("sdv real data saved to: ", real_data_csv)
    print("sdv trained model saved to: ", sdv_model_file)
    print("sdv sampled data saved to: ", sampled_data_csv)
    print("sdv synthetic data saved to: ", synthetic_data_csv)

    import my_arg

    arg_ = my_arg.parse_arg()

    # training
    epochs = 256

    # sampling
    num_sequence = 2 # ie number of "artificial" thing (ie year if sequence key is year, created from date column)
    sequence_len = 365 * 24 # does not have to be 365


    # The SDV uses the pandas library for data manipulation and synthesizing

    # WARNING: also CLI for solar2ev
    features_input_csv = config_features.features_input_csv 

    print("\n\nbuilding SDV synthetic timeseries data\n\n")

    features_used = []

    for x in config_features.config["input_feature_list"]: # possibly a list of list
        features_used = features_used + list(x) # is tuple
    features_used = list(set(features_used)) # all features in all heads
    
    print("features used as network input", features_used)


    ################
    # build real data and metadata
    # real data contains all features used in all heads, plus year (added) or month as sequence key
    #   n years of data
    ################

    # wTF. function modifies feature_used
    # If we pass list as parameter to a function and change the parameter, the original list is also changed. This is because list is a mutable type,

    metadata, real_data, sequence_key_col = build_metadata_realdata(features_input_csv, features_used)

    # year colums + all colums in network input except trig

    ############
    # train sdv on real data (or load already trained model)
    ############
    if os.path.exists(sdv_model_file):
        print("\n!!!! WARNING: sdv trained model %s already exist. reuse it" %sdv_model_file)
        synthesizer = PARSynthesizer.load(filepath=sdv_model_file)

    else:
        # 3sec / iteration, ie epochs. 256 epochs
        # SDV model fitted in 644 sec. 3 sec/epochs. ~15mn
        print("\ntrain and save sdv model. %d epochs, sequence key: %s. features: %s" %(epochs, sequence_key_col, real_data.columns.to_list()))
        synthesizer = train_synthetic(real_data, metadata , epochs=epochs)



    ############
    # create synthetic data by postprocessing sampled data
    # reuse sampled data if availabled  
    ############

    # sequence , ie fake year
    # note: hard to interpret an artificial month, they are not the same
    # artificial year (or sequence of 365 consecutive days) may be more meaningfull
    # len, arbitrary, does not have to be 30 or 365
        

    #################
    # sampling is very long (1h), reuse if already done
    #################

    if os.path.exists(sampled_data_csv):
        print("\n!!!! WARNING: sampled data %s already exist. reuse it" %sampled_data_csv)
        sampled_data = pd.read_csv(sampled_data_csv)

        for c in sampled_data.columns:
            d = sampled_data[c]
            print("loaded sampled data %s: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(c, d.max(), d.min(), d.mean(), d.std()))

        sdv_report_plot_analyze(real_data, sampled_data, metadata)


    else:
        print("\nsample new synthetic data to: %s" %sampled_data_csv)

        # sequence_len = 365 * 24 , ie one full year

        if debug:
            num_sequence = 1
            sequence_len = 40

        sampled_data = sample_synthetic(synthesizer, num_sequences=num_sequence, sequence_length=sequence_len) 


        ########################
        # analyze sampled data
        ########################

        for c in sampled_data.columns:

            if c in ["temp", "humid", "pressure", "wind"]:

                d = sampled_data[c]
                print("sampled data %s: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(c, d.max(), d.min(), d.mean(), d.std()))

                (_,p) = shapiro(d)
                if p<0.05:
                    print("%s is not normal" %c)
                else:
                    print("%s is normal" %c)


        ###### save raw sampled data
        # WARNING: csv file contains the year column. not a problem as this is not selected when creating dataset (and post processe below)
        sampled_data.to_csv(sampled_data_csv, header=True, index=False)
        print("sampled data csv %s available" %sampled_data_csv)

        ##### PLOT sampled data
        # not easy, syn data does not contains date

        ###############
        # all quality, diagnostics reports
        ###############
        all_reports(sampled_data)


    # sampled data is available

    ###################
    # postprocess sampled data to create a df which looks like feature input use for training
    # drop year
    # create month , trig(month) if was used
    # create hour 
    # if direction was included, create trig
    ###################

    print("postprocess sampled data, to look like network input")

    syn_data = post_process(sampled_data)
    print("synthetic (postprocessed) data:", syn_data.columns)

    for c in syn_data.columns:
        d = syn_data[c]
        print("synthetic (post processed sampled) %s: max %0.1f, min %0.1f, mean %0.1f, std %0.1f" %(c, d.max(), d.min(), d.mean(), d.std()))

    ################
    # save syn_data (to be used by training)
    ################
        
    syn_data.to_csv(synthetic_data_csv, header=True, index=False)

    print("%d hours, %d days %0.1f years" %(len(syn_data), len(syn_data)/24, len(syn_data)/(365*24)))


    ################
    # validate before calling it a day
    ################
    check_syn_data()

    print("\n#########\nsynthetic data %s ready for use\n#########\n" %synthetic_data_csv)
    print("end of sdv")
